{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe486f2e",
   "metadata": {},
   "source": "# Fall‑risk prediction on the gait analysis data\n\nThis notebook demonstrates how to load the combined gait dataset, preprocess it, handle class imbalance, train multiple models (Logistic Regression, Random Forest and XGBoost) **optimized for ROC AUC**, and evaluate them on a hold‑out test set.\n\n## Key Features:\n- **Hyperparameter optimization using GridSearchCV/RandomizedSearchCV with ROC AUC as the scoring metric**\n- 5-fold cross-validation with ROC AUC scoring\n- Comprehensive evaluation including ROC curves and confusion matrices\n- Model comparison based on ROC AUC performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1044351",
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ded99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('combined_output.csv')\n",
    "# Map labels to binary\n",
    "y = df['Faller'].map({'F': 1, 'NF': 0})\n",
    "# Drop ID and label, convert to numeric and fill missing values\n",
    "X = df.drop(columns=['ID', 'Faller']).apply(pd.to_numeric, errors='coerce')\n",
    "X = X.fillna(X.median())\n",
    "print('Dataset shape:', X.shape)\n",
    "print('Class distribution:', y.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe13383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/test with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Oversample the minority class in the training set\n",
    "train_df = X_train.copy()\n",
    "train_df['label'] = y_train\n",
    "majority = train_df[train_df['label'] == 0]\n",
    "minority = train_df[train_df['label'] == 1]\n",
    "minority_over = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "train_bal = pd.concat([majority, minority_over])\n",
    "X_train_bal = train_bal.drop(columns=['label'])\n",
    "y_train_bal = train_bal['label']\n",
    "\n",
    "print('Balanced training set shape:', X_train_bal.shape)\n",
    "print('Balanced class distribution:', y_train_bal.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f029f81",
   "metadata": {},
   "outputs": [],
   "source": "# Logistic Regression with GridSearchCV optimizing for ROC AUC\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_bal)\nX_test_scaled = scaler.transform(X_test)\n\n# Define parameter grid for Logistic Regression\nparam_grid_lr = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga'],\n    'max_iter': [1000]\n}\n\n# GridSearchCV with ROC AUC scoring\nlog_reg_cv = GridSearchCV(\n    LogisticRegression(),\n    param_grid_lr,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1\n)\nlog_reg_cv.fit(X_train_scaled, y_train_bal)\n\nprint('Best parameters for Logistic Regression:', log_reg_cv.best_params_)\nprint('Best cross-validation ROC AUC score:', log_reg_cv.best_score_)\n\n# Use best model for predictions\nlog_reg = log_reg_cv.best_estimator_\ny_pred_lr = log_reg.predict(X_test_scaled)\ny_prob_lr = log_reg.predict_proba(X_test_scaled)[:,1]\n\nacc_lr = accuracy_score(y_test, y_pred_lr)\nprec_lr = precision_score(y_test, y_pred_lr, zero_division=0)\nrec_lr = recall_score(y_test, y_pred_lr)\nf1_lr = f1_score(y_test, y_pred_lr)\nauc_lr = roc_auc_score(y_test, y_prob_lr)\n\nprint('\\nLogistic Regression Test Set Results:')\nprint('Accuracy:', acc_lr)\nprint('Precision:', prec_lr)\nprint('Recall:', rec_lr)\nprint('F1:', f1_lr)\nprint('ROC AUC:', auc_lr)\n\n# Confusion matrix\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nsns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted NF','Predicted F'], yticklabels=['True NF','True F'])\nplt.title('Logistic Regression Confusion Matrix')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8c53cb",
   "metadata": {},
   "outputs": [],
   "source": "# Random Forest with RandomizedSearchCV optimizing for ROC AUC\nparam_dist_rf = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [None, 10, 20, 30, 40],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['sqrt', 'log2'],\n    'bootstrap': [True, False],\n    'class_weight': ['balanced', 'balanced_subsample', None]\n}\n\n# RandomizedSearchCV with ROC AUC scoring\nrf_cv = RandomizedSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_dist_rf,\n    n_iter=50,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\nrf_cv.fit(X_train_bal, y_train_bal)\n\nprint('Best parameters for Random Forest:', rf_cv.best_params_)\nprint('Best cross-validation ROC AUC score:', rf_cv.best_score_)\n\n# Use best model for predictions\nrf = rf_cv.best_estimator_\ny_pred_rf = rf.predict(X_test)\ny_prob_rf = rf.predict_proba(X_test)[:,1]\n\nacc_rf = accuracy_score(y_test, y_pred_rf)\nprec_rf = precision_score(y_test, y_pred_rf, zero_division=0)\nrec_rf = recall_score(y_test, y_pred_rf)\nf1_rf = f1_score(y_test, y_pred_rf)\nauc_rf = roc_auc_score(y_test, y_prob_rf)\n\nprint('\\nRandom Forest Test Set Results:')\nprint('Accuracy:', acc_rf)\nprint('Precision:', prec_rf)\nprint('Recall:', rec_rf)\nprint('F1:', f1_rf)\nprint('ROC AUC:', auc_rf)\n\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nsns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted NF','Predicted F'], yticklabels=['True NF','True F'])\nplt.title('Random Forest Confusion Matrix')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b96a0",
   "metadata": {},
   "outputs": [],
   "source": "# XGBoost with RandomizedSearchCV optimizing for ROC AUC\nparam_dist_xgb = {\n    'n_estimators': [100, 200, 300, 500],\n    'max_depth': [3, 4, 5, 6, 7],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'gamma': [0, 0.1, 0.5, 1],\n    'min_child_weight': [1, 3, 5],\n    'scale_pos_weight': [1, 2, 3]  # For handling class imbalance\n}\n\n# RandomizedSearchCV with ROC AUC scoring\nxgb_cv = RandomizedSearchCV(\n    XGBClassifier(random_state=42, eval_metric='auc', use_label_encoder=False),\n    param_dist_xgb,\n    n_iter=50,\n    cv=5,\n    scoring='roc_auc',\n    n_jobs=-1,\n    verbose=1,\n    random_state=42\n)\nxgb_cv.fit(X_train_bal, y_train_bal)\n\nprint('Best parameters for XGBoost:', xgb_cv.best_params_)\nprint('Best cross-validation ROC AUC score:', xgb_cv.best_score_)\n\n# Use best model for predictions\nxgb_model = xgb_cv.best_estimator_\ny_pred_xgb = xgb_model.predict(X_test)\ny_prob_xgb = xgb_model.predict_proba(X_test)[:,1]\n\nacc_xgb = accuracy_score(y_test, y_pred_xgb)\nprec_xgb = precision_score(y_test, y_pred_xgb, zero_division=0)\nrec_xgb = recall_score(y_test, y_pred_xgb)\nf1_xgb = f1_score(y_test, y_pred_xgb)\nauc_xgb = roc_auc_score(y_test, y_prob_xgb)\n\nprint('\\nXGBoost Test Set Results:')\nprint('Accuracy:', acc_xgb)\nprint('Precision:', prec_xgb)\nprint('Recall:', rec_xgb)\nprint('F1:', f1_xgb)\nprint('ROC AUC:', auc_xgb)\n\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\nsns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted NF','Predicted F'], yticklabels=['True NF','True F'])\nplt.title('XGBoost Confusion Matrix')\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2bb6e",
   "metadata": {},
   "outputs": [],
   "source": "# Summarize results in a DataFrame\nresults = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],\n    'CV ROC AUC': [log_reg_cv.best_score_, rf_cv.best_score_, xgb_cv.best_score_],\n    'Test ROC AUC': [auc_lr, auc_rf, auc_xgb],\n    'Test Accuracy': [acc_lr, acc_rf, acc_xgb],\n    'Test Precision': [prec_lr, prec_rf, prec_xgb],\n    'Test Recall': [rec_lr, rec_rf, rec_xgb],\n    'Test F1': [f1_lr, f1_rf, f1_xgb]\n})\nresults = results.sort_values('Test ROC AUC', ascending=False).reset_index(drop=True)\nprint('\\n=== Model Performance Summary (Optimized for ROC AUC) ===')\nresults"
  },
  {
   "cell_type": "code",
   "id": "8tylh62lwoo",
   "source": "# ROC Curve Comparison for all models\nplt.figure(figsize=(10, 8))\n\n# Calculate ROC curves for each model\nfpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\nfpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\nfpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)\n\n# Plot ROC curves\nplt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {auc_lr:.3f})', linewidth=2)\nplt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.3f})', linewidth=2)\nplt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.3f})', linewidth=2)\n\n# Plot diagonal reference line\nplt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1)\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate', fontsize=12)\nplt.ylabel('True Positive Rate', fontsize=12)\nplt.title('ROC Curves - Model Comparison', fontsize=14)\nplt.legend(loc='lower right', fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint('\\nROC AUC Summary:')\nprint(f'Logistic Regression: {auc_lr:.4f}')\nprint(f'Random Forest: {auc_rf:.4f}')\nprint(f'XGBoost: {auc_xgb:.4f}')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}